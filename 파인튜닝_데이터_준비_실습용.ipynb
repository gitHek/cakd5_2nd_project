{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"파인튜닝_데이터_준비_실습용.ipynb","provenance":[{"file_id":"1gb1b09oLW4ikiwzpPUYMFl4-qZOdJSt0","timestamp":1650951918788}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Finetuning input Data 준비 코드 실습 \n","\n","강의자 : 김재원\n","\n","* 이 단계를 진행하기 위해서는 Finetuning을 위해 준비한 data.txt 파일이 필요합니다. 아직 data.txt 파일이 준비되지 않았다면, 먼저 데이터를 생성하고 진행해주세요.\n","\n","* 다음 코드는 꼭 순서대로 실행시켜주셔야 tensorflow 버전 error가 나지 않습니다. \n","\n","* 코드를 실행시키기 전 제공해 드린 tokenizationK.py 코드와 vocab file \"vocab.korean.rawtext.list\" 를 최상위 경로에 업로드 시켜주세요."],"metadata":{"id":"iki6ZordJ1k5"}},{"cell_type":"code","source":["## downgrade tensorflow version\n","%tensorflow_version 1.x"],"metadata":{"id":"vI4XGk1LBbB7","executionInfo":{"status":"ok","timestamp":1650956988216,"user_tz":-540,"elapsed":5,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"_edptXvSq2o8"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ux5aiLTiZ_EA","executionInfo":{"status":"ok","timestamp":1650956990596,"user_tz":-540,"elapsed":2384,"user":{"displayName":"권혁종","userId":"07484803130759059282"}},"outputId":"9c15c8a3-da45-4560-935f-61559fa28897"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab_Notebooks/2nd_project/dataset')"],"metadata":{"id":"blGfvaAaZ57R","executionInfo":{"status":"ok","timestamp":1650956990596,"user_tz":-540,"elapsed":6,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"jZDHk9lwANAl","executionInfo":{"status":"ok","timestamp":1650956990596,"user_tz":-540,"elapsed":6,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"outputs":[],"source":["import os, re\n","import argparse\n","\n","# get tokenizer python script\n","from tokenizationK import FullTokenizer"]},{"cell_type":"code","source":["## get tokenizer vocab file\n","tokenizer = FullTokenizer(vocab_file=\"/content/drive/MyDrive/Colab_Notebooks/2nd_project/dataset/vocab.korean.rawtext.list\")"],"metadata":{"id":"-MGbgSqTAS01","executionInfo":{"status":"ok","timestamp":1650956990596,"user_tz":-540,"elapsed":5,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["문제 1) \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\"와 같은 예시처럼\n","해당 데이터에서는 \"/슬롯(레이블)명;엔티티/\"의 형식으로 슬롯과 엔티티를 정리해 놨으므로,\n","이를 잡아 줄 수 있는 정규표현식을 준비한다.\n","\n","* 힌트0. 정규표현식 recap\n","- https://colab.research.google.com/drive/1YH77oCCBLMP6yIUHf-BQozLCxWmRbdd7\n","  위의 경로에서 정규표현식 부분을 다시 보시고 작성하면 됩니다.\n","\n","* 힌트1. slot_pattern\n","  - text1 = \"/인물;한지민/\" \n","  - text2 = \"/인물;한예슬/\" \n","  - text3 = \"/가수;아이유/\"  \n","  위의 3개의 text를 모두 잡을 수 있는 정규표현식이다.\n","  기본적인 형식은 /OO;OO/ 이므로 OO에 들어가는 모든 문자를 가져오는 정규표현식을 찾아보면 된다.\n","\n","* 힌트2. multi_spaces\n","  - multi_spaces는 말그대로 multiple한 space를 잡는 정규표현식이다. 참고로 정규표현식에서 space는 \"\\s\"이다. \n","  \n"],"metadata":{"id":"2dXLUJvLBqld"}},{"cell_type":"code","source":["## ----------------- 문제 1 ---------------- ##\n","slot_pattern = re.compile('/(.+?);(.+?)/')\n","multi_spaces = re.compile('\\s+')\n","## ---------------------------------------- ##"],"metadata":{"id":"2bO1YPyHBq6x","executionInfo":{"status":"ok","timestamp":1650956990597,"user_tz":-540,"elapsed":6,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### process_file : seq.in, seq.out 생성\n","단방향 데이터가 있는 file_path를 argument로 주면 가공을 한 이후에 output_dir 아래의 2개의 파일(seq.in, seq.out)을 저장해 주는 함수 "],"metadata":{"id":"wtushuGgEAgG"}},{"cell_type":"markdown","source":["문제 2) tokens와 tags를 각각 processed_data에서 list로 뽑기\n","\n","* 힌트. process_file class는 process_line 함수를 사용한다.따라서 process_line 함수의 output을 출력해보고, process_data의 형식을 안다면, lambda 함수를 사용해 tokens와 tags를 list로 출력할 수 있다. \n"],"metadata":{"id":"8hYLsW9FCHJf"}},{"cell_type":"code","source":["def process_file(file_path, output_dir):\n","  if not os.path.isdir(output_dir):\n","    os.mkdir(output_dir)\n","  \n","  data = open(file_path).read().splitlines()\n","\n","  ## line별로 processing\n","  processed_data = [process_line(line, tokenizer) for line in data]\n","\n","\n","  ## ----------------- 문제 2 ---------------- ##\n","  tokens = list(map(lambda  x : x[0],processed_data))\n","  tags = list(map(lambda  x : x[1],processed_data))\n","  ## ---------------------------------------- ##\n","\n","  ## seq_in : 토큰들로만 이루어진 파일\n","  ## seq_out : 태그들로만 이루어진 파일\n","  seq_in = os.path.join(output_dir, \"seq.in\")\n","  seq_out = os.path.join(output_dir, \"seq.out\")\n","\n","  with open(seq_in, \"w\") as f:\n","    f.write(\"\\n\".join(tokens)+ \"\\n\")\n","\n","  with open(seq_out, \"w\") as f:\n","    f.write(\"\\n\".join(tags)+ \"\\n\")"],"metadata":{"id":"klzMBWV8B7wV","executionInfo":{"status":"ok","timestamp":1650956990597,"user_tz":-540,"elapsed":5,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### process_line : 데이터를 라인별로 처리해주는 함수\n","sentence를 input으로 받아 (token, tag)쌍으로 된 결과값을 반환한다.\n","예를 들어 \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\" 같은 input을 받게 되면,\n","        ('한 지민 과 한예 슬 나오 는 드라마 있 어 ?', '인물 인물 O 인물 인물 O O O O O O')와 같은 (토큰, 태그)쌍 반환\n","\n","문제 3) 토큰의 개수와 슬롯의 개수가 맞지 않는다면 본래 라인과 더불어 토큰/슬롯들을 프린트해주는 코드를 작성하시오.\n","\n","* 힌트. print문 안에 있는 내용을 살펴보고, 필요한 것들을 가져다가 if문으로 비교를 하시면 됩니다. "],"metadata":{"id":"IcI17OixD8QJ"}},{"cell_type":"code","source":["def process_line(sentence, tokenizer):\n","  slot_pattern_found = slot_pattern.findall(sentence)\n","  line_refined = slot_pattern.sub(\"/슬롯/\", sentence)\n","  tokens = \"\"\n","  tags = \"\"\n","  slot_index = 0\n","\n","  for word in line_refined.split():\n","    ## \"/게임명;일곱개의 대죄/\" --> (\"게임명\", \"일곱개의 대죄\")\n","    if word.startswith(\"/\"):\n","      slot, entity = slot_pattern_found[slot_index]\n","      slot_index += 1\n","\n","      # 줄 들여쓰기가 잘못되어있는거 같아 한칸 뒤로 밈\n","      ## 엔티티를 토크나이즈 한 후, 토큰별로 태그를 추가해준다. \n","      entity_tokens = \" \".join(tokenizer.tokenize(entity))\n","\n","      tokens += entity_tokens + \" \"\n","      tags += (slot + \" \") * len(entity_tokens.split())\n","\n","      ## 조사가 붙은 것이며(eg. \"/슬롯/이\", \"/슬롯/에서\"),\n","      ## 조사에 대해서 추가적으로 토큰 및 태그를 추가해 준다.\n","      if not word.endswith(\"/\"):\n","        ## 우선 \"/\" 뒤에 오는 조사를 찾아 준다.\n","        josa = word[word.rfind(\"/\")+1:]\n","        josa_tokens = \" \".join(tokenizer.tokenize(josa))\n","\n","        tokens += josa_tokens + \" \"\n","        tags += \"O \" * len(josa_tokens.split())\n","    \n","    elif \"/\" in word:\n","      prefix = word.split(\"/\")[0]\n","      tokenized_prefix = \" \".join(tokenizer.tokenize(prefix))\n","      tokens += tokenized_prefix + \" \"\n","      tags += \"O \" * len(tokenized_prefix.split())\n","\n","      slot, entity = slot_pattern_found[slot_index]\n","      slot_index += 1\n","\n","      entity_tokens = \" \".join(tokenizer.tokenize(entity))\n","\n","      tokens += entity_tokens + \" \"\n","      tags += (slot + \" \") * len(entity_tokens.split())\n","\n","    else:\n","      word_tokens = \" \".join(tokenizer.tokenize(word))\n","      tokens += word_tokens + \" \"\n","      tags += \"O \" * len(word_tokens.split())\n","  \n","  tokens = multi_spaces.sub(\" \", tokens.strip())\n","  tags = multi_spaces.sub(\" \", tags.strip())\n","\n","  ## 만일 토큰의 개수와 슬롯의 개수가 맞지 않다면 본래 라인과 더불어 토큰/슬롯들을 프린트해준다.\n","  ## ----------------- 문제 3 ---------------- ##\n","  if len(tokens.split()) != len(tags.split()):    \n","        print(sentence)\n","        print(\"\\t\" + tokens + \"\\t\", len(tokens.split()))\n","        print(\"\\t\" + tags + \"\\t\", len(tags.split()))\n","  ## ---------------------------------------- ##\n","  \n","  return tokens, tags"],"metadata":{"id":"-mLsa9yIEZOV","executionInfo":{"status":"ok","timestamp":1650956990597,"user_tz":-540,"elapsed":4,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["file_path = '/content/drive/MyDrive/Colab_Notebooks/2nd_project/dataset/data.txt'\n","output_dir = '/content/drive/MyDrive/Colab_Notebooks/2nd_project/dataset'"],"metadata":{"id":"jclwcc4TtIO5","executionInfo":{"status":"ok","timestamp":1650957167166,"user_tz":-540,"elapsed":1,"user":{"displayName":"권혁종","userId":"07484803130759059282"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["process_file(file_path, output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":327},"id":"YZChYKhUtPZB","executionInfo":{"status":"error","timestamp":1650957167641,"user_tz":-540,"elapsed":8,"user":{"displayName":"권혁종","userId":"07484803130759059282"}},"outputId":"0c502cdb-78f7-41c5-910c-3bcb8931c15b"},"execution_count":24,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-36e2b9b39c22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-04badf951766>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(file_path, output_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m## line별로 processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-04badf951766>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m## line별로 processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprocess_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-3ff1b852c99a>\u001b[0m in \u001b[0;36mprocess_line\u001b[0;34m(sentence, tokenizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# \"/게임명;일곱개의 대죄/\" --> (\"게임명\", \"일곱개의 대죄\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mslot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_pattern_found\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslot_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mslot_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"markdown","source":["### 주의사항"],"metadata":{"id":"6uILg9HKJtlf"}},{"cell_type":"markdown","source":["위의 코드를 참조해서 input/output argument를 받는 python script를 생성해주세요. python script의 이름은 prepare_data.py입니다. \n","주의할 점은 tokenizer와 vocab file은 제공해드린 것을 사용해주세요.\n","\n","* 아래의 주석 내용은 argparse의 기본적인 내용입니다. 참조해서 python 파일을 만들어주시면 됩니다.\n","\n","* 또한 python script를 생성하고 나서는 tensorflow 버전과 python 버전을 잘 체크해주셔야 합니다. \n","\n","  - python version 3.7\n","  - tensorflow version 1.15\n","\n","* 로컬이나 환경을 생성하셔서 위와 같이 버전을 맞춰주시고 코드를 돌리시면 무리없이 잘 돌아갈 것입니다. \n","\n","  - $ python prepare_data.py --input data.txt --output output\n","\n","* 위와 같은 코드를 입력하셔서 python script를 돌리실 수 있습니다. 자세한 내용은 아래 argparse를 참조해주세요. \n","\n","* 수요일에는 prepare_data.py 정답 코드를 제공해드리겠습니다. 우선은 colab으로 함수의 구조를 익혀보시고 직접 script를 짜보시길 바랍니다. 궁금한 점이 있다면 카톡방에 물어봐주세요! "],"metadata":{"id":"Y_k5XcdgJUC1"}},{"cell_type":"code","source":["import argparse\n","\n","# 인자값을 받을 수 있는 인스턴스 생성\n","parser = argparse.ArgumentParser(description='사용법 테스트입니다.')\n","\n","# 입력받을 인자값 등록\n","parser.add_argument('--target', required=True, help='어느 것을 요구하냐')\n","parser.add_argument('--env', required=False, default='dev', help='실행환경은 뭐냐')\n","\n","# 입력받은 인자값을 args에 저장 (type: namespace)\n","args = parser.parse_args()\n","\n","# 입력받은 인자값 출력\n","print(args.target)\n","print(args.env)"],"metadata":{"id":"KfFsu1CzJRSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# $ python3 argparse_test.py\n","# usage: argparse_test.py [-h] --target TARGET [--env ENV]\n","# argparse_test.py: error: the following arguments are required: --target\n","\n","# $ python3 argparse_test.py -h\n","# usage: argparse_test.py [-h] --target TARGET [--env ENV]\n","\n","# 사용법 테스트입니다.\n","\n","# optional arguments:\n","#   -h, --help       show this help message and exit\n","#   --target TARGET  어느 것을 요구하냐\n","#   --env ENV        실행환경은 뭐냐"],"metadata":{"id":"Kngaxh5VJSWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"41MVqCe6JYVn"},"execution_count":null,"outputs":[]}]}