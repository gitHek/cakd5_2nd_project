{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "파인튜닝_데이터_준비_이현정.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHek/cakd5_2nd_project_2team/blob/develop/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A4%80%EB%B9%84_%EC%9D%B4%ED%98%84%EC%A0%95_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning input Data 준비 코드 실습 \n",
        "\n",
        "강의자 : 김재원\n",
        "\n",
        "* 이 단계를 진행하기 위해서는 Finetuning을 위해 준비한 data.txt 파일이 필요합니다. 아직 data.txt 파일이 준비되지 않았다면, 먼저 데이터를 생성하고 진행해주세요.\n",
        "\n",
        "* 다음 코드는 꼭 순서대로 실행시켜주셔야 tensorflow 버전 error가 나지 않습니다. \n",
        "\n",
        "* 코드를 실행시키기 전 제공해 드린 tokenizationK.py 코드와 vocab file \"vocab.korean.rawtext.list\" 를 최상위 경로에 업로드 시켜주세요."
      ],
      "metadata": {
        "id": "iki6ZordJ1k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## downgrade tensorflow version\n",
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "vI4XGk1LBbB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EiCa-GmhRaZ",
        "outputId": "7e29b095-6f78-467c-d059-e87b6c1ac600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/cakd5_colab')"
      ],
      "metadata": {
        "id": "k92eLSEbiMg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZDHk9lwANAl"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import argparse\n",
        "\n",
        "# get tokenizer python script\n",
        "from tokenizationK import FullTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## get tokenizer vocab file\n",
        "tokenizer = FullTokenizer(vocab_file=\"/content/drive/MyDrive/cakd5_colab/vocab.korean.rawtext.list\")"
      ],
      "metadata": {
        "id": "-MGbgSqTAS01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 1) \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\"와 같은 예시처럼\n",
        "해당 데이터에서는 \"/슬롯(레이블)명;엔티티/\"의 형식으로 슬롯과 엔티티를 정리해 놨으므로,\n",
        "이를 잡아 줄 수 있는 정규표현식을 준비한다.\n",
        "\n",
        "* 힌트0. 정규표현식 recap\n",
        "- https://colab.research.google.com/drive/1YH77oCCBLMP6yIUHf-BQozLCxWmRbdd7\n",
        "  위의 경로에서 정규표현식 부분을 다시 보시고 작성하면 됩니다.\n",
        "\n",
        "* 힌트1. slot_pattern\n",
        "  - text1 = \"/인물;한지민/\" \n",
        "  - text2 = \"/인물;한예슬/\" \n",
        "  - text3 = \"/가수;아이유/\"\n",
        "  위의 3개의 text를 모두 잡을 수 있는 정규표현식이다.\n",
        "  기본적인 형식은 /OO;OO/ 이므로 OO에 들어가는 모든 문자를 가져오는 정규표현식을 찾아보면 된다.\n",
        "\n",
        "* 힌트2. multi_spaces\n",
        "  - multi_spaces는 말그대로 multiple한 space를 잡는 정규표현식이다. 참고로 정규표현식에서 space는 \"\\s\"이다. \n",
        "  \n"
      ],
      "metadata": {
        "id": "2dXLUJvLBqld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ----------------- 문제 1 ---------------- ##\n",
        "import re\n",
        "slot_pattern = re.compile('/(.+?);(.+?)/')\n",
        "multi_spaces = re.compile('\\s+')\n",
        "## ---------------------------------------- ##"
      ],
      "metadata": {
        "id": "2bO1YPyHBq6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# slot_pattern = re.compile('/[가-힣]+;[가-힣]+/')도 가능"
      ],
      "metadata": {
        "id": "tOgOI-HvBlod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =  \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\""
      ],
      "metadata": {
        "id": "AakFssPYV5sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slot_pattern_found = slot_pattern.findall(sentence)\n",
        "slot_pattern_found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0R3Ex71V1fN",
        "outputId": "9120972e-99eb-406e-e004-63afb2eb50d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('인물', '한지민'), ('인물', '한예슬')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line_refined = slot_pattern.sub(\"/슬롯/\", sentence)\n",
        "line_refined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_Fm6TU3_a7W9",
        "outputId": "c3fcba26-ea86-49ca-91d0-a5d9ae19d3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/슬롯/과 /슬롯/ 나오는 드라마 있어?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity = \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\""
      ],
      "metadata": {
        "id": "izxmJnKDbNdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slot_index = 0\n",
        "slot, entity = slot_pattern_found[slot_index]"
      ],
      "metadata": {
        "id": "ZNl_PYA2bVmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_tokens = \" \".join(tokenizer.tokenize(entity))\n",
        "tokens = \"\"\n",
        "tags = \"\"\n",
        "tokens += entity_tokens + \" \"\n",
        "tags += (slot + \" \") * len(entity_tokens.split())"
      ],
      "metadata": {
        "id": "1VnAQvl7bKQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HgS9EGxqbfBt",
        "outputId": "ef041e3b-b5e7-4cd9-c69b-ec95d52a6e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'한 지 민_ '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlA_nBG8b3F4",
        "outputId": "de3ee798-b9fd-4250-c5d7-4db44c73b101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uYVaDfDbqwo",
        "outputId": "bfa84e6a-0c01-4b79-a54f-da621718e6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yn1KD5Sfbgas",
        "outputId": "61da8530-ae10-4f43-9430-93521773a483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'인물 인물 인물 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tags.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZRtbY2SbsB9",
        "outputId": "fef7453b-a45d-46c2-9675-86e489be8744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = sentence.splitlines()\n",
        "processed_data = [process_line(line, tokenizer) for line in data]\n",
        "processed_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMkkvIczfpaf",
        "outputId": "a56d74db-7183-4435-9deb-627c0bd7ec65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('한 지 민_ 과_ 한 예 슬 _ 나오는_ 드라마_ 있어 ?_', '인물 인물 인물 O 인물 인물 인물 인물 O O O O')]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = list(map(lambda x: x[0],processed_data))\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrivmck6fnsU",
        "outputId": "b40b0488-b16e-41db-f6d5-f534b09e2cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['한 지 민_ 과_ 한 예 슬 _ 나오는_ 드라마_ 있어 ?_']"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#apply(lambda) 를 쓸 경우 str에는 apply가 안 된다며 에러가 남 -> map으로 수정"
      ],
      "metadata": {
        "id": "TkxYbprkBxFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### process_file : seq.in, seq.out 생성\n",
        "단방향 데이터가 있는 file_path를 argument로 주면 가공을 한 이후에 output_dir 아래의 2개의 파일(seq.in, seq.out)을 저장해 주는 함수 "
      ],
      "metadata": {
        "id": "wtushuGgEAgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 2) tokens와 tags를 각각 processed_data에서 list로 뽑기\n",
        "\n",
        "* 힌트. process_file class는 process_line 함수를 사용한다.따라서 process_line 함수의 output을 출력해보고, process_data의 형식을 안다면, lambda 함수를 사용해 tokens와 tags를 list로 출력할 수 있다. \n"
      ],
      "metadata": {
        "id": "8hYLsW9FCHJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_path, output_dir):\n",
        "  if not os.path.isdir(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "  \n",
        "  data = open(file_path).read().splitlines()\n",
        "\n",
        "  ## line별로 processing\n",
        "  processed_data = [process_line(line, tokenizer) for line in data]\n",
        "\n",
        "\n",
        "  ## ----------------- 문제 2 ---------------- ##\n",
        "  tokens = list(map(lambda x: x[0],processed_data))\n",
        "  tags = list(map(lambda x: x[1],processed_data))\n",
        "  ## ---------------------------------------- ##\n",
        "\n",
        "  ## seq_in : 토큰들로만 이루어진 파일\n",
        "  ## seq_out : 태그들로만 이루어진 파일\n",
        "  seq_in = os.path.join(output_dir, \"seq.in\")\n",
        "  seq_out = os.path.join(output_dir, \"seq.out\")\n",
        "\n",
        "  with open(seq_in, \"w\") as f:\n",
        "    f.write(\"\\n\".join(tokens)+ \"\\n\")\n",
        "\n",
        "  with open(seq_out, \"w\") as f:\n",
        "    f.write(\"\\n\".join(tags)+ \"\\n\")"
      ],
      "metadata": {
        "id": "klzMBWV8B7wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### process_line : 데이터를 라인별로 처리해주는 함수\n",
        "sentence를 input으로 받아 (token, tag)쌍으로 된 결과값을 반환한다.\n",
        "예를 들어 \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\" 같은 input을 받게 되면,\n",
        "        ('한 지민 과 한예 슬 나오 는 드라마 있 어 ?', '인물 인물 O 인물 인물 O O O O O O')와 같은 (토큰, 태그)쌍 반환\n",
        "\n",
        "문제 3) 토큰의 개수와 슬롯의 개수가 맞지 않는다면 본래 라인과 더불어 토큰/슬롯들을 프린트해주는 코드를 작성하시오.\n",
        "\n",
        "* 힌트. print문 안에 있는 내용을 살펴보고, 필요한 것들을 가져다가 if문으로 비교를 하시면 됩니다. "
      ],
      "metadata": {
        "id": "IcI17OixD8QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_line(sentence, tokenizer):\n",
        "  slot_pattern_found = slot_pattern.findall(sentence)\n",
        "  line_refined = slot_pattern.sub(\"/슬롯/\", sentence)\n",
        "  tokens = \"\"\n",
        "  tags = \"\"\n",
        "  slot_index = 0\n",
        "\n",
        "  for word in line_refined.split():\n",
        "    ## \"/게임명;일곱개의 대죄/\" --> (\"게임명\", \"일곱개의 대죄\")\n",
        "    if word.startswith(\"/\"):\n",
        "      slot, entity = slot_pattern_found[slot_index]\n",
        "      slot_index += 1\n",
        "\n",
        "    ## 엔티티를 토크나이즈 한 후, 토큰별로 태그를 추가해준다. \n",
        "      entity_tokens = \" \".join(tokenizer.tokenize(entity))\n",
        "\n",
        "      tokens += entity_tokens + \" \"\n",
        "      tags += (slot + \" \") * len(entity_tokens.split())\n",
        "\n",
        "      ## 조사가 붙은 것이며(eg. \"/슬롯/이\", \"/슬롯/에서\"),\n",
        "    ## 조사에 대해서 추가적으로 토큰 및 태그를 추가해 준다.\n",
        "      if not word.endswith(\"/\"):\n",
        "        ## 우선 \"/\" 뒤에 오는 조사를 찾아 준다.\n",
        "        josa = word[word.rfind(\"/\")+1:]\n",
        "        josa_tokens = \" \".join(tokenizer.tokenize(josa))\n",
        "\n",
        "        tokens += josa_tokens + \" \"\n",
        "        tags += \"O \" * len(josa_tokens.split())\n",
        "    \n",
        "    elif \"/\" in word:\n",
        "      prefix = word.split(\"/\")[0]\n",
        "      tokenized_prefix = \" \".join(tokenizer.tokenize(prefix))\n",
        "      tokens += tokenized_prefix + \" \"\n",
        "      tags += \"O \" * len(tokenized_prefix.split())\n",
        "\n",
        "      slot, entity = slot_pattern_found[slot_index]\n",
        "      slot_index += 1\n",
        "\n",
        "      entity_tokens = \" \".join(tokenizer.tokenize(entity))\n",
        "\n",
        "      tokens += entity_tokens + \" \"\n",
        "      tags += (slot + \" \") * len(entity_tokens.split())\n",
        "\n",
        "    else:\n",
        "      word_tokens = \" \".join(tokenizer.tokenize(word))\n",
        "      tokens += word_tokens + \" \"\n",
        "      tags += \"O \" * len(word_tokens.split())\n",
        "  \n",
        "  tokens = multi_spaces.sub(\" \", tokens.strip())\n",
        "  tags = multi_spaces.sub(\" \", tags.strip())\n",
        "\n",
        "  ## 만일 토큰의 개수와 슬롯의 개수가 맞지 않다면 본래 라인과 더불어 토큰/슬롯들을 프린트해준다.\n",
        "  ## ----------------- 문제 3 ---------------- ##\n",
        "  # [if 문으로 시작하는 코드 작성 ! ] \n",
        "  if len(tokens.split())!= len(tags.split()):\n",
        "\n",
        "    print(sentence)\n",
        "    print(\"\\t\" + tokens + \"\\t\", len(tokens.split()))\n",
        "    print(\"\\t\" + tags + \"\\t\", len(tags.split()))\n",
        "    print('\\t'+slot+'\\t',len(slot.split()))\n",
        "  ## ---------------------------------------- ##\n",
        "  \n",
        "  return tokens, tags"
      ],
      "metadata": {
        "id": "-mLsa9yIEZOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#len(tokens)!= len(tags) 로만 하면 길이의 차이가 발생, split()을 추가하면 길이 항상 동일 "
      ],
      "metadata": {
        "id": "_3U9PM-CB8km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_line( \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\", tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HatgXvINkmf-",
        "outputId": "d73cbdda-3568-4161-c419-c553f0772615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('한 지 민_ 과_ 한 예 슬 _ 나오는_ 드라마_ 있어 ?_', '인물 인물 인물 O 인물 인물 인물 인물 O O O O')"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 주의사항"
      ],
      "metadata": {
        "id": "6uILg9HKJtlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 코드를 참조해서 input/output argument를 받는 python script를 생성해주세요. python script의 이름은 prepare_data.py입니다. \n",
        "주의할 점은 tokenizer와 vocab file은 제공해드린 것을 사용해주세요.\n",
        "\n",
        "* 아래의 주석 내용은 argparse의 기본적인 내용입니다. 참조해서 python 파일을 만들어주시면 됩니다.\n",
        "\n",
        "* 또한 python script를 생성하고 나서는 tensorflow 버전과 python 버전을 잘 체크해주셔야 합니다. \n",
        "\n",
        "  - python version 3.7\n",
        "  - tensorflow version 1.15\n",
        "\n",
        "* 로컬이나 환경을 생성하셔서 위와 같이 버전을 맞춰주시고 코드를 돌리시면 무리없이 잘 돌아갈 것입니다. \n",
        "\n",
        "  - $ python prepare_data.py --input data.txt --output output\n",
        "\n",
        "* 위와 같은 코드를 입력하셔서 python script를 돌리실 수 있습니다. 자세한 내용은 아래 argparse를 참조해주세요. \n",
        "\n",
        "* 수요일에는 prepare_data.py 정답 코드를 제공해드리겠습니다. 우선은 colab으로 함수의 구조를 익혀보시고 직접 script를 짜보시길 바랍니다. 궁금한 점이 있다면 카톡방에 물어봐주세요! "
      ],
      "metadata": {
        "id": "Y_k5XcdgJUC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os, re\n",
        "import argparse\n",
        "from tokenizationK import FullTokenizer\n",
        "\n",
        "import pdb\n",
        "\n",
        "############################################## TODO 경로 고치기 ###############################################\n",
        "tokenizer = FullTokenizer(vocab_file=\"/content/drive/MyDrive/cakd5_colab/vocab.korean.rawtext.list\")\n",
        "###############################################################################################################\n",
        "\n",
        "# \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\"와 같은 예시처럼\n",
        "# 해당 데이터에서는 \"/슬롯(레이블)명;엔티티/\"의 형식으로 슬롯과 엔티티를 정리해 놨으므로,\n",
        "# 이를 잡아 줄 수 있는 정규표현식을 준비한다.\n",
        "\n",
        "slot_pattern = re.compile('/(.+?);(.+?)/')\n",
        "multi_spaces = re.compile('\\s+')\n",
        "\n",
        "\n",
        "def process_file(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    단방향 데이터가 있는 file_path을 argument로 주면 가공을 한 이후에\n",
        "    output_dir 아래에 2개의 파일(seq.in, seq.out)을 저장해 주는 함수.\n",
        "    \"\"\"\n",
        "    if not os.path.isdir(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    data = open(file_path).read().splitlines()\n",
        "\n",
        "    # line별로 process를 해준 뒤,\n",
        "    processed_data = [process_line(line, tokenizer) for line in data]\n",
        "    ## tokens = [알맞은 코드를 써주세요]\n",
        "    ## tags = [알맞은 코드를 써주세요]\n",
        "\n",
        "    # seq_in : 토큰들로만 이루어진 파일\n",
        "    # seq_out : 태그들로만 이루어진 파일\n",
        "    seq_in = os.path.join(output_dir, \"seq.in\")\n",
        "    seq_out = os.path.join(output_dir, \"seq.out\")\n",
        "\n",
        "    with open(seq_in, \"w\") as f:\n",
        "        f.write(\"\\n\".join(tokens)+ \"\\n\")\n",
        "\n",
        "    with open(seq_out, \"w\") as f:\n",
        "        f.write(\"\\n\".join(tags)+ \"\\n\")\n",
        "\n",
        "\n",
        "def process_line(sentence, tokenizer):\n",
        "    \"\"\"\n",
        "    데이터를 라인별로 처리해 주는 함수이다.\n",
        "    라인을 주게 되면, (토큰, 슬롯)\n",
        "\n",
        "    예를 들어 \"/인물;한지민/과 /인물;한예슬/ 나오는 드라마 있어?\" 같은 input을 받게 되면,\n",
        "        ('한 지민 과 한예 슬 나오 는 드라마 있 어 ?', '인물 인물 O 인물 인물 O O O O O O')와 같은 (토큰, 태그)쌍으로 된 결과값을 반환한다.\n",
        "    \"\"\"\n",
        "    slot_pattern_found = slot_pattern.findall(sentence)\n",
        "    line_refined = slot_pattern.sub(\"/슬롯/\", sentence)\n",
        "    tokens = \"\"\n",
        "    tags = \"\"\n",
        "    slot_index = 0\n",
        "\n",
        "    for word in line_refined.split():\n",
        "        # \"/게임명;일곱개의 대죄/\" --> (\"게임명\", \"일곱개의 대죄\")\n",
        "        if word.startswith(\"/\"):\n",
        "            slot, entity = slot_pattern_found[slot_index]\n",
        "            slot_index += 1\n",
        "\n",
        "            # 엔티티를 토크나이즈 한 후, 토큰별로 태그를 추가해 준다.\n",
        "            entity_tokens = \" \".join(tokenizer.tokenize(entity))\n",
        "\n",
        "            tokens += entity_tokens + \" \"\n",
        "            tags += (slot + \" \") * len(entity_tokens.split())\n",
        "\n",
        "            # 조사가 붙은 것이며(eg. \"/슬롯/이\", \"/슬롯/에서\"),\n",
        "            # 조사에 대해서 추가적으로 토큰 및 태그를 추가해 준다.\n",
        "            if not word.endswith(\"/\"):\n",
        "                # 우선 \"/\" 뒤에 오는 조사를 찾아 주고\n",
        "                josa = word[word.rfind(\"/\")+1:]\n",
        "                josa_tokens = \" \".join(tokenizer.tokenize(josa))\n",
        "\n",
        "                tokens += josa_tokens + \" \"\n",
        "                tags += \"O \" * len(josa_tokens.split())\n",
        "            \n",
        "        elif \"/\" in word:\n",
        "\n",
        "            prefix = word.split(\"/\")[0]\n",
        "            tokenized_prefix = \" \".join(tokenizer.tokenize(prefix))\n",
        "            tokens += tokenized_prefix + \" \"\n",
        "            tags += \"O \" * len(tokenized_prefix.split())\n",
        "\n",
        "            slot, entity = slot_pattern_found[slot_index]\n",
        "            slot_index += 1\n",
        "\n",
        "            entity_tokens = \" \".join(tokenizer.tokenize(entity))\n",
        "\n",
        "            tokens += entity_tokens + \" \"\n",
        "            tags += (slot + \" \") * len(entity_tokens.split())\n",
        "\n",
        "        else:\n",
        "            word_tokens = \" \".join(tokenizer.tokenize(word))\n",
        "\n",
        "            tokens += word_tokens + \" \"\n",
        "            tags += \"O \" * len(word_tokens.split())\n",
        "\n",
        "    tokens = multi_spaces.sub(\" \", tokens.strip())\n",
        "    tags = multi_spaces.sub(\" \", tags.strip())\n",
        "\n",
        "    # 만일 토큰의 개수와 슬롯의 개수가 맞지 않다면 본래 라인과 더불어 토큰/슬롯들을 프린트해준다.\n",
        "    if len(tokens.split())!= len(tags.split()):\n",
        "        print(sentence)\n",
        "        print(\"\\t\" + tokens + \"\\t\", len(tokens.split()))\n",
        "        print(\"\\t\" + tags + \"\\t\", len(tags.split()))\n",
        "\n",
        "    return tokens, tags\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--input_dir\", \"-i\", help = \"Etri KorBERT dir\", type = str, required=True)\n",
        "    parser.add_argument(\"--output_dir\", \"-o\", help = \"output_dir\", type = str, required=True)\n",
        "\n",
        "   # [ i nput argument를 받는 코드를 추가해주세요]\n",
        "   # [ ouput argument를 받는 코드를 추가해주세요]\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    file_path = args.input\n",
        "    output_dir = args.output\n",
        "\n",
        "    process_file(file_path, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "1VyG_hIBA4de",
        "outputId": "91955459-78ef-4833-87ac-abbe4f1d1373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] --input_dir INPUT_DIR --output_dir\n",
            "                             OUTPUT_DIR\n",
            "ipykernel_launcher.py: error: the following arguments are required: --input_dir/-i, --output_dir/-o\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip list"
      ],
      "metadata": {
        "id": "472MUa-Vpez9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3g5t4H3_zSL4",
        "outputId": "98658571-a5f1-4825-aad9-49c3acde0b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "# # # 인자값을 받을 수 있는 인스턴스 생성\n",
        "# parser = argparse.ArgumentParser(description='사용법 테스트입니다.')\n",
        "\n",
        "# # # 입력받을 인자값 등록\n",
        "# parser.add_argument('--target', required=True, help='어느 것을 요구하냐')\n",
        "# #옵션을 지정하지 않아도 되는 인수\n",
        "# parser.add_argument('--env', required=False, default='dev', help='실행환경은 뭐냐')\n",
        "\n",
        "# # # 입력받은 인자값을 args에 저장 (type: namespace)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# # # 입력받은 인자값 출력\n",
        "# print(args.target)\n",
        "# print(args.env)"
      ],
      "metadata": {
        "id": "KfFsu1CzJRSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !$ python prepare_data.py --input data.txt --output output\n",
        "# usage: argparse_test.py [-h] --target TARGET [--env ENV]\n",
        "# argparse_test.py: error: the following arguments are required: --target\n",
        "\n",
        "# !$ python prepare_data.py --input data.txt --output output -h\n",
        "# usage: argparse_test.py [-h] --target TARGET [--env ENV]\n",
        "\n",
        "# # 사용법 테스트입니다.\n",
        "\n",
        "# optional arguments:\n",
        "#   -h, --help       show this help message and exit\n",
        "#   --target TARGET  어느 것을 요구하냐\n",
        "#   --env ENV        실행환경은 뭐냐"
      ],
      "metadata": {
        "id": "Kngaxh5VJSWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}